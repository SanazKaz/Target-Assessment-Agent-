{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "import asyncio  # Add this import\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import json\n",
    "from APIs.combinedapi import PubMedProcessor\n",
    "%autoawait asyncio\n",
    "import datetime\n",
    "from IPython.display import display, HTML\n",
    "from difflib import unified_diff\n",
    "import weave \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "openai_client = OpenAI(api_key=\"OPENAI_API_KEY\")\n",
    "load_dotenv()\n",
    "llm = ChatOpenAI()\n",
    "llm = ChatOpenAI(api_key=\"OPENAI_API_KEY\" )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_prompt = \"You are ChatGPT, a large language model trained by OpenAI, based on the GPT-4 architecture and you cite the papers you use in your answers using Harvard Style.\"\n",
    "\n",
    "working_hypothesis_prompt = \"\"\" \n",
    "# Scientific Rationale for 5-alpha reductase  in Huntington's Disease\n",
    "\n",
    "\n",
    "## Target Information \n",
    "### Develop a scientific rationale for the following:\n",
    "                             \n",
    "    **Given target:**  5-alpa reductase\n",
    "    **Given disease:** Huntington's disease\n",
    "    **Given mode of action:** Inhibition of 5-alpha reductase can promote neuronal survival and reduce mutant huntingtin protein aggregation\n",
    "\n",
    "## Task 1: Develop Scientific Rationale\n",
    "\n",
    "### Working Hypothesis\n",
    "- Detailed description of the idea\n",
    "- Unmet medical need\n",
    "- Suitability for combination therapy\n",
    "- Predictive biomarkers\n",
    "- Clinical relevance of existing biomarkers\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "clinical_target_prompt = \"\"\" #Clinical Target Rationale for 5-alpha reductase in Huntington's Disease\n",
    "\n",
    "\n",
    "## Target Information \n",
    "### Develop a scientific rationale for the following:\n",
    "                           \n",
    "    **Given target:** 5-alpa reductase\n",
    "    **Given disease:** Huntington's disease\n",
    "    **Given mode of action:** Inhibition of 5-alpha reductase can promote neuronal survival and reduce mutant huntingtin protein aggregation\n",
    "\n",
    "\n",
    " ### Clinical target rationale:\n",
    "    - How relevant is the target location to the disease biology?\n",
    "    - How is the target expression altered in human disease?\n",
    "    - How is the target involved in the physiological process relevant to the disease?\n",
    "    - Which phenotypes and genotypes were identified for the target?\n",
    "    - How is the genetic link between the target and the disease?\n",
    "    - Describe the evidence provided in clinics or by tools acting on the pathway where the target is involved.\n",
    "    - Which kind of target modulation is required to treat the disease? \"\"\"\n",
    "\n",
    "Challenges_prompt_1 = \"\"\" \n",
    "#Challenges for the drug discovery program related to 5-alpha reductase  in Huntington's disease\n",
    "                           \n",
    "## Target Information \n",
    "### Develop a scientific rationale for the following:\n",
    "                           \n",
    "    **Given target:**  5-alpa reductase\n",
    "    **Given disease:** Huntington's disease\n",
    "    **Given mode of action:** Inhibition of 5-alpha reductase can promote neuronal survival and reduce mutant huntingtin protein aggregation\n",
    "\n",
    "\n",
    "### Challenges:\n",
    "- Check the following idea for details on small molecule compounds: Developing small molecule modulators or inhibitors of gamma secretase for Alzheimer's disease treatment.\n",
    "- Is a 'information driven approach' (IDA) strategy based on available small molecules possible?\n",
    "- Which small molecular modulators of the target known?\n",
    "- Which inhibitors, antagonists, agonists, negative allosteric modulators (NAM), positive allosteric modulators (PAM) are required for target modulation in the given disease? \n",
    "\n",
    "\"\"\"\n",
    "Challenges_prompt_2 = \"\"\"\n",
    "\n",
    "#Challenges for the drug discovery program related to 5-alpha reductase  in Huntington's disease\n",
    "                           \n",
    "## Target Information \n",
    "### Develop a scientific rationale for the following:\n",
    "                           \n",
    "    **Given target:**  5-alpa reductase\n",
    "    **Given disease:** Huntington's disease\n",
    "    **Given mode of action:** Inhibition of 5-alpha reductase can promote neuronal survival and reduce mutant huntingtin protein aggregation\n",
    "\n",
    "- Which patients would respond the therapy?\n",
    "- Is the proposed mode of action on the target desirable and commercially viable in a clinical setting?\n",
    "- What are advantages and disadvantages of different therapeutic modalities (antibodies, small molecules, antisense oligonucleotides, PROTACs, molecular glue, peptide macrocycles, and so on) for tackling the target?\n",
    "\n",
    "- Alternative indications:\n",
    "- Describe alternative indication for modulators of the target and explain why.\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "all_prompts = [working_hypothesis_prompt, clinical_target_prompt, Challenges_prompt_1, Challenges_prompt_2]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import Dict, Any\n",
    "import openai\n",
    "import asyncio\n",
    "from typing import Dict, Any\n",
    "\n",
    "\n",
    "async def pubmed_paperqa(query: str) -> Dict[str, Any]:\n",
    "    \"\"\" Searches PubmedCentral for papers using a query\n",
    "    and returns the most relevant chunks using paperQA\"\"\"\n",
    "\n",
    "    max_attempts = 2\n",
    "    max_results: int = 4\n",
    "    pubmed_query = query\n",
    "    doc_query = query\n",
    "    email = \"sanazkazemi@hotmail.com\"\n",
    "    print(f\"pubmed_paperqa called with query: {query}, max_results: {max_results}\")\n",
    "\n",
    "    \n",
    "    pubmed_instance = PubMedProcessor(email)\n",
    "\n",
    "    # some error handeling in case the API call fails the algorithm will continue.\n",
    "    results_dict = {}\n",
    "    for attempt in range(max_attempts):\n",
    "        try:\n",
    "            results_dict = await pubmed_instance.full_process(pubmed_query, doc_query, max_results)\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"Error in attempt {attempt+1}: {e}\")\n",
    "\n",
    "    if results_dict is None:\n",
    "        print(\"All API call attempts failed. Continuing with empty results.\")\n",
    "        results_dict = {\"error\": \"API calls failed\", \"results\": []}\n",
    "\n",
    "\n",
    "    # saving the refs to file because the LLM cant do it as it edits in paragraphs - has to be manually done\n",
    "\n",
    "    file_path = 'RATT_refs.json'\n",
    "\n",
    "    try:\n",
    "        if os.path.exists(file_path) and os.path.getsize(file_path) > 0:\n",
    "            with open(file_path, 'r') as f:\n",
    "                existing_data = json.load(f)\n",
    "        else:\n",
    "            existing_data = []\n",
    "    except json.JSONDecodeError:\n",
    "        print(\"Error reading existing data. Starting with empty list.\")\n",
    "\n",
    "        existing_data = []\n",
    "\n",
    "    existing_data.append(results_dict)\n",
    "\n",
    "    with open(file_path, 'w') as f:\n",
    "        json.dump(existing_data, f, indent=4)\n",
    "\n",
    "                        \n",
    "    return json.dumps(results_dict, indent=4)\n",
    "\n",
    "# To run this in a Jupyter notebook cell:\n",
    "# query = \"Alzheimer's disease and gamma secretase\"\n",
    "# results = await pubmed_paperqa(query)\n",
    "# print(results)\n",
    "\n",
    "from difflib import unified_diff\n",
    "import html\n",
    "\n",
    "def generate_diff_html(text1, text2, fromfile='Original', tofile='Modified'):\n",
    "    diff = unified_diff(text1.splitlines(keepends=True),\n",
    "                        text2.splitlines(keepends=True),\n",
    "                        fromfile=fromfile, tofile=tofile, n=3)\n",
    "    \n",
    "    html_output = ['''\n",
    "    <style>\n",
    "        .diff-container {\n",
    "            font-family: monospace;\n",
    "            white-space: pre-wrap;\n",
    "            word-wrap: break-word;\n",
    "            background-color: #f8f9fa;\n",
    "            border: 1px solid #dee2e6;\n",
    "            border-radius: 4px;\n",
    "            padding: 10px;\n",
    "            margin-bottom: 20px;\n",
    "        }\n",
    "        .diff-header {\n",
    "            color: #6c757d;\n",
    "            margin-bottom: 10px;\n",
    "        }\n",
    "        .diff-add {\n",
    "            background-color: #e6ffec;\n",
    "            color: #24292e;\n",
    "        }\n",
    "        .diff-sub {\n",
    "            background-color: #ffebe9;\n",
    "            color: #24292e;\n",
    "        }\n",
    "        .diff-line {\n",
    "            display: block;\n",
    "            margin-bottom: 0;\n",
    "            padding: 2px 0;\n",
    "        }\n",
    "        .collapse-button {\n",
    "            background-color: #007bff;\n",
    "            color: white;\n",
    "            border: none;\n",
    "            padding: 5px 10px;\n",
    "            margin-bottom: 10px;\n",
    "            cursor: pointer;\n",
    "            border-radius: 4px;\n",
    "        }\n",
    "        .hidden {\n",
    "            display: none;\n",
    "        }\n",
    "    </style>\n",
    "    <div class=\"diff-container\">\n",
    "    <button class=\"collapse-button\" onclick=\"toggleDiff(this)\">Collapse/Expand Diff</button>\n",
    "    <div class=\"diff-content\">\n",
    "    ''']\n",
    "    \n",
    "    for line in diff:\n",
    "        if line.startswith('---') or line.startswith('+++'):\n",
    "            html_output.append(f'<div class=\"diff-header\">{html.escape(line)}</div>')\n",
    "        elif line.startswith('+'):\n",
    "            html_output.append(f'<span class=\"diff-line diff-add\">{html.escape(line)}</span>')\n",
    "        elif line.startswith('-'):\n",
    "            html_output.append(f'<span class=\"diff-line diff-sub\">{html.escape(line)}</span>')\n",
    "        else:\n",
    "            html_output.append(f'<span class=\"diff-line\">{html.escape(line)}</span>')\n",
    "    \n",
    "    html_output.append('''\n",
    "    </div>\n",
    "    </div>\n",
    "    <script>\n",
    "    function toggleDiff(button) {\n",
    "        var content = button.nextElementSibling;\n",
    "        if (content.style.display === \"none\") {\n",
    "            content.style.display = \"block\";\n",
    "            button.textContent = \"Collapse Diff\";\n",
    "        } else {\n",
    "            content.style.display = \"none\";\n",
    "            button.textContent = \"Expand Diff\";\n",
    "        }\n",
    "    }\n",
    "    </script>\n",
    "    ''')\n",
    "    \n",
    "    return ''.join(html_output)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def pick_best_query(queries: list, question: str, answer: str) -> str:\n",
    "    \"\"\"picks the best query from the list of queries\"\"\"\n",
    "\n",
    "    gpt_prompt =\"\"\"\n",
    "    for the given question and answer, pick the best query \n",
    "    from the list of queries that you think is most relevant especially to the last few sentences of the answer.\n",
    "    ## IMPORTANT:\n",
    "    Just return the best query. Do not add any additional information.\n",
    "    \"\"\"\n",
    "\n",
    "    best_query = openai.chat.completions.create(\n",
    "\n",
    "     model = \"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"you are a scientific researcher, you are tasked with finding the best query to search for scientific papers on PubMed.\"            \n",
    "        },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"##Instruction:{gpt_prompt}\\n\\n###Question: {question}\\n\\n###Question:{answer}\\n\\n##Queries: {[queries]}\\n\\n\"\n",
    "            }\n",
    "        ],\n",
    "        temperature=1 # here you can adjust the temperature to get more or less creative search terms\n",
    "    ).choices[0].message.content\n",
    "\n",
    "    return best_query\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_query(question, answer, num_queries) -> str:\n",
    "\n",
    "    \"\"\"Generates queries to search for in Pubmed based on the question\"\"\"    \n",
    "    query_prompt = \"\"\" You are a scientific researcher, \n",
    "                        you are tasked with finding the best query to search \n",
    "                        for scientific papers on PubMed.\n",
    "                                                    \n",
    "                            I want to verify the content correctness of the given answer especially the last few sentences.\n",
    "                            Please summarize the content with the corresponding question.\n",
    "                            This summarization will be used as a query to search with Bing search engine.\n",
    "                            The query should be short but needs to be specific to promise Bing can find related knowledge or pages.\n",
    "                            You can also use search syntax to make the query short and clear enough for the search engine to find relevant language data.\n",
    "                            Try to make the query as relevant as possible to the last few setences of the the answer provided.\n",
    "                            **IMPORTANT**\n",
    "                            Just output the query directly. DO NOT add additional explanations or introducement in the answer unless you are asked to.\n",
    "\n",
    "                        The following worked very well for me in the past in terms of generating the highest number of results use it as a guide:\n",
    "                        ###Example:\n",
    "                        \"{Target}\" AND \"{Disease}\" AND (\"{relevant_keyword}\" OR \"{relevant_keyword_1} Or \"{relevant_keyword_n}\")\" and so on.\n",
    "                        ##IMPORTANT:\n",
    "                        Just provide the query. Do not add any additional information.\n",
    "                        DO NOT copy the given example\"\"\"\n",
    "    \n",
    "\n",
    "    queries = []\n",
    "\n",
    "    for i in range(num_queries):\n",
    "        try:\n",
    "            query = openai.chat.completions.create(\n",
    "                model=\"gpt-4o-mini\",\n",
    "                messages=[\n",
    "                    {\n",
    "                        \"role\": \"system\",\n",
    "                        \"content\": \"You are ChatGPT, a large language model trained by OpenAI, based on the GPT-4 architecture.\"\n",
    "\n",
    "                    },\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": f\"##Question: {question}\\n\\n##Content: {answer}\\n\\n##Instruction: {query_prompt}\"\n",
    "                    }\n",
    "                ],\n",
    "                temperature=1\n",
    "            ).choices[0].message.content\n",
    "\n",
    "            print(f\"query {i}: {query}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"error {e}\")\n",
    "        queries.append(query)\n",
    "\n",
    "    best_query = pick_best_query(queries,answer, question)\n",
    "    print(f\"best query: {best_query}\")\n",
    "\n",
    "    return best_query\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "async def main(question: str, answer, num_queries: int):\n",
    "    \"\"\"Main function to get the best query for the question\"\"\"\n",
    "    \n",
    "    \n",
    "    search_query = get_query(question, answer, num_queries)\n",
    "    \n",
    "    # Remove only the outermost single quotes if they exist otherwise doesnt work - not elegant but works\n",
    "    if search_query.startswith(\"'\") and search_query.endswith(\"'\"):\n",
    "        cleaned_query = search_query[1:-1]\n",
    "    else:\n",
    "        cleaned_query = search_query\n",
    "    \n",
    "    # Replace escaped single quotes with regular single quotes\n",
    "    cleaned_query = cleaned_query.replace(\"\\\\'\", \"'\")\n",
    "    \n",
    "    print(f\"Cleaned search query: {cleaned_query}\")\n",
    "    \n",
    "    results = await pubmed_paperqa(cleaned_query)\n",
    "\n",
    "    print(results)  # This is the final output\n",
    "\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = \"\"\" notch signalling is relevant to GSEC development and drug discovery in Alzheimer's disease.\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = await main(answer=answer, question = working_hypothesis_prompt, num_queries=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_agents = 1 \n",
    "num_steps = 3\n",
    "final_output_mode = 'final_step_only'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "def COT_agent(question):\n",
    "    \"\"\"Generates a chain of thought answer for comparison to RATT.\n",
    "    question: str: the prompt to answer\n",
    "    draft_prompt: str: the prompt to generate the draft\n",
    "    system_prompt: str: the prompt to generate the system message\"\"\"\n",
    "\n",
    "    draft_prompt = '''\n",
    "IMPORTANT:\n",
    "Try to answer this question/instruction with step-by-step thoughts and make the answer more structured.\n",
    "Use `\\n\\n` to split the answer into several paragraphs.\n",
    "Just respond to the instruction directly. DO NOT add additional explanations or introducement in the answer unless you are asked to.\n",
    "'''\n",
    "\n",
    "    # Loop to generate different initial answers\n",
    "    COT_draft = openai.chat.completions.create(\n",
    "         model=\"gpt-4o-mini\",\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": chat_prompt # should this be scientific rationale prompt or something less specific?\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": question + draft_prompt\n",
    "                }\n",
    "            ],\n",
    "            temperature=0.5\n",
    "        ).choices[0].message.content\n",
    "\n",
    "    return COT_draft\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "\n",
    "def split_draft(draft, split_char='\\n\\n'):\n",
    "    # split_char: '\\n\\n'\n",
    "    draft_paragraphs = draft.split(split_char)\n",
    "    # print(f\"The draft answer has {len(draft_paragraphs)}\")\n",
    "    return draft_paragraphs\n",
    "\n",
    "\n",
    "def get_revise_answer(question, answer, retrieved_data):\n",
    "    revise_prompt = '''\n",
    "I want to revise the answer according to retrieved related text of the question from Pubmed central. You will receive the summary, full text, relevance score to the query, and the full reference. If you use any of the given articles, you **must** reference using the Harvard style with DOI added and add the full citation to the top of the answer. \n",
    "**DO NOT REMOVE OR ALTER ANY CITATIONS FROM THE TOP OF THE ANSWER.**\n",
    "You need to check whether the answer is correct.\n",
    "If you find some errors in the answer, revise the answer to make it better.\n",
    "If you find some necessary details are ignored, add it to make the answer more plausible according to the related text.\n",
    "If you find that a part of the answer is correct and does not require any additional details, maintain that part of the answer unchanged. Directly output the original content of that part without any modifications.\n",
    "**IMPORTANT**\n",
    "Try to keep the structure (multiple paragraphs with its subtitles) in the revised answer and make it more structual for understanding.\n",
    "Split the paragraphs with `\\n\\n` characters.\n",
    "Just output the revised answer directly. DO NOT add additional explanations or annoucement in the revised answer unless you are asked to.\n",
    "'''\n",
    "    revised_answer = openai.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": chat_prompt\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"##Pubmed central retrieved articles: {retrieved_data}\\n\\n##Question: {question}\\n\\n##previous Answer: {answer}\\n\\n##Instruction: {revise_prompt}\"\n",
    "            }\n",
    "        ],\n",
    "        temperature=0.5\n",
    "    ).choices[0].message.content\n",
    "    \n",
    "    \n",
    "    return revised_answer\n",
    "\n",
    "\n",
    "\n",
    "async def RAG(question, draft_paragraphs):\n",
    "    \"\"\" args:\n",
    "    question: str: the prompt to answer\n",
    "    draft_paragraphs: list: the list of paragraphs from the initial n drafts\n",
    "    \"\"\"\n",
    "    answer = \"\"\n",
    "\n",
    "    for i, paragraph in enumerate(draft_paragraphs):\n",
    "        answer += '\\n\\n' + paragraph\n",
    "\n",
    "        api_response = await main(question, answer, num_queries=2)  # Now using the entire answer instead of just the paragraph\n",
    "\n",
    "        revised_answer = get_revise_answer(question, answer, api_response)  # Using the entire answer\n",
    "        if revised_answer != answer:\n",
    "            diff_html = generate_diff_html(answer, revised_answer)\n",
    "            display(HTML(diff_html))\n",
    "            answer = revised_answer\n",
    "        \n",
    "        print(f\"Completed iteration {i+1}/{len(draft_paragraphs)}\")\n",
    "\n",
    "        print('+'* 80 + '\\n\\n')\n",
    "        print(f\"RESULT OF PUBMED API:\\n{answer}\")\n",
    "    \n",
    "    return answer\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "async def get_draft_tot_initial(question: str, num_agents: int):\n",
    "    \"\"\"Generates initial answers from multiple agents for comparison\"\"\"\n",
    "    draft_prompt = \"\"\"\n",
    "            IMPORTANT:\n",
    "            Try to answer this question/instruction with step-by-step thoughts and make the answer more structured.\n",
    "            Use `\\n\\n` to split the answer into several paragraphs.\n",
    "            Just respond to the instruction directly. DO NOT add additional explanations or introducement in the answer unless you are asked to.\n",
    "            \"\"\"\n",
    "\n",
    "    refine_prompt = \"\"\"\n",
    "            Maintaining *ALL* citations and references and referencing the answers provided by all agents, synthesize a more detailed and comprehensive response by integrating all relevant details from these answers. \n",
    "            Ensure logical coherence and provide ONLY THE MERGED ANSWER AND CITATIONS AS THE OUTPUT, omitting any discussion of the comparison process or analytical thoughts.\"\"\"\n",
    "\n",
    "    agent_drafts = []\n",
    "    for i in range(num_agents):\n",
    "        draft = openai.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": chat_prompt\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": question + draft_prompt\n",
    "                }\n",
    "            ],\n",
    "            temperature=0.5\n",
    "        ).choices[0].message.content\n",
    "        print(f\"####################draft {i}: {draft}########################################\")\n",
    "\n",
    "        print(\"Processing draft...\")\n",
    "        draft_paragraphs = split_draft(draft)\n",
    "\n",
    "        draft_modified = await RAG(question, draft_paragraphs)\n",
    "\n",
    "        agent_drafts.append(f\"Agent{i+1}: {draft_modified}\")\n",
    "\n",
    "        print(f\"[INFO] Agent{i + 1}/{num_agents} retrieved draft...\")\n",
    "\n",
    "        agent_input = '\\n\\n'.join(agent_drafts) + '\\n\\n' + refine_prompt\n",
    "\n",
    "        final_draft = openai.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": chat_prompt\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": agent_input\n",
    "                }\n",
    "            ],\n",
    "            temperature=0.5\n",
    "        ).choices[0].message.content\n",
    "\n",
    "        print(f\"{datetime.now()} - Final draft: {final_draft}\")\n",
    "\n",
    "\n",
    "    return final_draft\n",
    "\n",
    "# FIX BELOW FUNCTION NOT USING PREVIOUS ANSWER ?? ALSO STRING CHAR LIST WITHIN LIST - SHOULD BE STRING\n",
    "\n",
    "\n",
    "async def get_draft_tot(question, previous_answer, num_agents):\n",
    "\n",
    "    draft_prompt = f\"\"\" Base your response on the provided question and the previous answer. Expand the answer by adding more details to enhance its comprehensiveness. Ensure that the expansion maintains logical coherence and enriches the details, making the response more thorough and well-structured.\n",
    "        Question: {question}\n",
    "        Previous Answer: {previous_answer}\n",
    "        IMPORTANT:\n",
    "        DO NOT REMOVE ANY CITATIONS OR REFERENCES IN THE ANSWER if you use any citations or references in the answer - you must reference in Harvard style + the DOI in the answer and add the citation to the top of the answer.\n",
    "        Answer the full question with step-by-step thoughts and make the answer more structural.\n",
    "        Use `\\n\\n` to split the answer into several paragraphs.\n",
    "        Just respond to the instruction directly. DO NOT add additional explanations or introducement in the answer unless you are asked to. \"\"\"\n",
    "\n",
    "    refine_prompt = \"\"\"Maintaining *ALL* citations and references and referencing the answers provided by all agents, synthesize a more detailed and comprehensive response by integrating all relevant details from these answers. \n",
    "            Ensure logical coherence and provide ONLY THE MERGED ANSWER AND CITATIONS AS THE OUTPUT, omitting any discussion of the comparison process or analytical thoughts.\"\"\"\n",
    "\n",
    "    agents_drafts = []\n",
    "    for i in range(num_agents):\n",
    "        draft = openai.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": chat_prompt\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": draft_prompt\n",
    "                }\n",
    "            ],\n",
    "            temperature=0.5\n",
    "        ).choices[0].message.content\n",
    "\n",
    "        draft_paragraphs = split_draft(draft)\n",
    "\n",
    "        draft_modified = await RAG(question, draft_paragraphs)\n",
    "\n",
    "        agents_drafts.append(f\"Agent{i+1}: {draft_modified}\")\n",
    "    \n",
    "    agents_input = '\\n\\n'.join(agents_drafts) + '\\n\\n' + refine_prompt\n",
    "\n",
    "    final_draft_raw = openai.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": chat_prompt\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": agents_input\n",
    "            }\n",
    "        ],\n",
    "        temperature=0.5\n",
    "    ).choices[0].message.content\n",
    "\n",
    "    print(f\"##########Final draft raw #########################: {final_draft_raw}...\")\n",
    "\n",
    "    revise_prompt = \"\"\"\n",
    "            Based on the original answer and an additional supplementary answer, generate a response that is richer in detail and logically coherent. MAINTAIN ALL REFERENCES AND CITATIONS IN THE ANSWER. Do not remove ANY citations.\n",
    "            Review the original answer:\n",
    "        1. If any part of the answer is correct and requires no further details, retain that portion unchanged and output it directly as it is.\n",
    "        2. For parts that may be improved or lack necessary details, enhance them by integrating information from the supplementary answer to make the response more comprehensive and accurate, reference and cite the full reference at the top where neccessary.\n",
    "        3. If you identify any errors within the answers, correct these errors while ensuring that the revised content remains logically coherent - check this against the retreived articles.\n",
    "        Original Answer: {previous_answer}\n",
    "        Supplementary Answer: {final_draft_raw}\n",
    "\n",
    "        **IMPORTANT**\n",
    "        Ensure the revised answer maintains a structured format with paragraphs and subtitles for better clarity. \n",
    "        Separate the paragraphs with `\\n\\n` characters. Output only the enhanced answer directly and the references and citations if any, without any extra explanations or announcements unless specifically requested.\"\"\"\n",
    "    \n",
    "    final_draft = openai.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": chat_prompt\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": revise_prompt\n",
    "            }\n",
    "        ],\n",
    "        temperature=0.5\n",
    "    ).choices[0].message.content\n",
    "\n",
    "    return final_draft\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "\n",
    "async def ratt(question, num_agents):\n",
    "    step_num = num_steps\n",
    "    print(f\"{datetime.now()} [INFO] Retrieving Step 1 draft...\")\n",
    "\n",
    "    draft = await get_draft_tot_initial(question,num_agents)\n",
    "    \n",
    "    print(f\"{datetime.now()} [INFO] Step 1 draft returned\")\n",
    "    print(f\"##################### DRAFT #######################\")\n",
    "    print(draft)\n",
    "    print(f\"#####################  END  #######################\")\n",
    "\n",
    "    print(f\"{datetime.now()} [INFO] Processing draft...\")\n",
    "    draft_paragraphs = split_draft(draft)\n",
    "    print(f\"{datetime.now()} [INFO] Draft split into {len(draft_paragraphs)} parts\")\n",
    "\n",
    "    answer_first_state = await RAG(question, draft_paragraphs)\n",
    "\n",
    "    previous_answer = answer_first_state\n",
    "\n",
    "    each_step_drafts = [f\"Step 1 \\n: {previous_answer}\"]\n",
    "\n",
    "    for iteration in range(1, step_num):\n",
    "        print(f\"{datetime.now()} [INFO] Retrieving Step {iteration + 1} draft...\")\n",
    "        draft = await get_draft_tot(question, previous_answer, num_agents=num_agents)\n",
    "        print(f\"{datetime.now()} [INFO] Step {iteration + 1} draft returned\")\n",
    "        print(f\"##################### DRAFT #######################\")\n",
    "        print(draft)\n",
    "        print(f\"#####################  END  #######################\")\n",
    "\n",
    "        print(f\"{datetime.now()} [INFO] Processing draft...\")\n",
    "        draft_paragraphs = split_draft(draft)\n",
    "        print(f\"{datetime.now()} [INFO] Draft split into {len(draft_paragraphs)} parts\")\n",
    "\n",
    "        # filtered_paragraphs = filter_paragraphs(draft_paragraphs, iteration, step_num)\n",
    "        final_answer = await RAG(question, draft_paragraphs)\n",
    "\n",
    "        each_step_drafts.append(f\"Step {iteration + 1} \\n: {final_answer}\")\n",
    "\n",
    "        # Update previous_answer for the current iteration's response\n",
    "        previous_answer = final_answer\n",
    "\n",
    "    draft_cot = COT_agent(question) # for comparison\n",
    "\n",
    "    if final_output_mode == 'combine_each_step':\n",
    "        final_draft = '\\n\\n'.join(each_step_drafts)\n",
    "        refine_prompt = f\"\"\"\n",
    "            Maintaining *ALL* citations and references and referencing the answers provided by all agents, synthesize a more detailed and comprehensive response by integrating all relevant details from these answers. \n",
    "            Ensure logical coherence and provide ONLY THE MERGED ANSWER AND CITATIONS AS THE OUTPUT, omitting any discussion of the comparison process or analytical thoughts.\"\"\"\n",
    "        previous_answer = openai.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": chat_prompt\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": final_draft + '\\n\\n' + refine_prompt\n",
    "                }\n",
    "            ],\n",
    "            temperature=0.5 ## ? why\n",
    "        ).choices[0].message.content\n",
    "\n",
    "    return draft_cot, previous_answer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import weave\n",
    "from weave import Evaluation\n",
    "import json\n",
    "import aiofiles\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# answer_cot, answer_ratt = ratt(chatgpt_prompt, num_agents=1)\n",
    "\n",
    "@weave.op()\n",
    "async def main_ratt(question, num_agents, output_file=\"text_ratt/EGFR/EGFR.json\"):\n",
    "    answer_cot, answer_ratt = await ratt(question, num_agents)\n",
    "\n",
    "    result = {\n",
    "        \"prompt\": question,\n",
    "        \"output_cot\": answer_cot, \n",
    "        \"output_ratt\": answer_ratt\n",
    "    }\n",
    "\n",
    "    # Write the result to a file\n",
    "    async with aiofiles.open(output_file, mode='a') as f:\n",
    "        await f.write(json.dumps(result) + \"\\n\")\n",
    "\n",
    "    return result\n",
    "\n",
    "weave.init('output_ratt_EGFR')\n",
    "\n",
    "result_2 = await main_ratt(Challenges_prompt_2, num_agents=1)\n",
    "# weave.init('output_ratt_bace')\n",
    "# for prompt in all_prompts:\n",
    "#     print(f\"**************************{prompt}****************************\")\n",
    "#     results = asyncio.run(main_ratt(prompt, num_agents=1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ignoring wrong pointing object 9 0 (offset 0)\n",
      "Ignoring wrong pointing object 19 0 (offset 0)\n",
      "Ignoring wrong pointing object 57 0 (offset 0)\n",
      "Ignoring wrong pointing object 128 0 (offset 0)\n",
      "Ignoring wrong pointing object 130 0 (offset 0)\n",
      "Ignoring wrong pointing object 132 0 (offset 0)\n",
      "Ignoring wrong pointing object 172 0 (offset 0)\n",
      "Ignoring wrong pointing object 174 0 (offset 0)\n",
      "Ignoring wrong pointing object 176 0 (offset 0)\n",
      "Ignoring wrong pointing object 263 0 (offset 0)\n",
      "Ignoring wrong pointing object 265 0 (offset 0)\n",
      "Ignoring wrong pointing object 267 0 (offset 0)\n",
      "Ignoring wrong pointing object 317 0 (offset 0)\n",
      "Ignoring wrong pointing object 319 0 (offset 0)\n",
      "Ignoring wrong pointing object 321 0 (offset 0)\n",
      "Ignoring wrong pointing object 366 0 (offset 0)\n",
      "Ignoring wrong pointing object 368 0 (offset 0)\n",
      "Ignoring wrong pointing object 370 0 (offset 0)\n",
      "Ignoring wrong pointing object 373 0 (offset 0)\n",
      "Ignoring wrong pointing object 392 0 (offset 0)\n",
      "Ignoring wrong pointing object 394 0 (offset 0)\n",
      "Ignoring wrong pointing object 396 0 (offset 0)\n",
      "Ignoring wrong pointing object 399 0 (offset 0)\n",
      "Ignoring wrong pointing object 455 0 (offset 0)\n",
      "Ignoring wrong pointing object 457 0 (offset 0)\n",
      "Ignoring wrong pointing object 459 0 (offset 0)\n",
      "Ignoring wrong pointing object 509 0 (offset 0)\n",
      "Ignoring wrong pointing object 511 0 (offset 0)\n",
      "Ignoring wrong pointing object 513 0 (offset 0)\n",
      "Ignoring wrong pointing object 578 0 (offset 0)\n",
      "Ignoring wrong pointing object 580 0 (offset 0)\n",
      "Ignoring wrong pointing object 582 0 (offset 0)\n",
      "Ignoring wrong pointing object 12 0 (offset 0)\n",
      "Ignoring wrong pointing object 17 0 (offset 0)\n",
      "Ignoring wrong pointing object 19 0 (offset 0)\n",
      "Ignoring wrong pointing object 25 0 (offset 0)\n",
      "Ignoring wrong pointing object 27 0 (offset 0)\n",
      "Ignoring wrong pointing object 88 0 (offset 0)\n",
      "Ignoring wrong pointing object 91 0 (offset 0)\n",
      "Ignoring wrong pointing object 132 0 (offset 0)\n",
      "Ignoring wrong pointing object 136 0 (offset 0)\n",
      "Ignoring wrong pointing object 167 0 (offset 0)\n",
      "Ignoring wrong pointing object 57 0 (offset 0)\n",
      "Ignoring wrong pointing object 59 0 (offset 0)\n",
      "Ignoring wrong pointing object 61 0 (offset 0)\n",
      "Ignoring wrong pointing object 64 0 (offset 0)\n",
      "Ignoring wrong pointing object 128 0 (offset 0)\n",
      "Ignoring wrong pointing object 155 0 (offset 0)\n",
      "Ignoring wrong pointing object 157 0 (offset 0)\n",
      "Ignoring wrong pointing object 159 0 (offset 0)\n",
      "Ignoring wrong pointing object 162 0 (offset 0)\n",
      "Ignoring wrong pointing object 167 0 (offset 0)\n",
      "Ignoring wrong pointing object 178 0 (offset 0)\n",
      "Ignoring wrong pointing object 180 0 (offset 0)\n",
      "Ignoring wrong pointing object 182 0 (offset 0)\n",
      "Ignoring wrong pointing object 185 0 (offset 0)\n",
      "Ignoring wrong pointing object 7 0 (offset 0)\n",
      "Ignoring wrong pointing object 198 0 (offset 0)\n",
      "Ignoring wrong pointing object 299 0 (offset 0)\n",
      "Ignoring wrong pointing object 301 0 (offset 0)\n",
      "Ignoring wrong pointing object 186 0 (offset 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 6 PDF documents.\n",
      "VectorStoreIndex created successfully.\n",
      "Index persisted to disk.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from llama_index.core import Document, VectorStoreIndex, Settings\n",
    "from langchain_openai import OpenAIEmbeddings, OpenAI\n",
    "from llama_index.core.node_parser import SentenceWindowNodeParser\n",
    "from llama_index.core.postprocessor import MetadataReplacementPostProcessor\n",
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "\n",
    "# Directory containing your PDFs\n",
    "pdf_directory = \"/Users/sanazkazeminia/Documents/LLM_Agent/Embedded_Articles/\"\n",
    "\n",
    "all_docs = []\n",
    "\n",
    "# Load all PDFs from the directory\n",
    "for filename in os.listdir(pdf_directory):\n",
    "    if filename.endswith(\".pdf\"):\n",
    "        pdf_path = os.path.join(pdf_directory, filename)\n",
    "        \n",
    "        loader = PyPDFLoader(pdf_path)\n",
    "        all_pages = loader.load()\n",
    "        \n",
    "        # Select specific pages (first 25 pages in this case)\n",
    "        selected_pages = all_pages\n",
    "        \n",
    "        # Combine the text from the selected pages\n",
    "        doc_text = \"\\n\\n\".join([page.page_content for page in selected_pages])\n",
    "        \n",
    "        # Create a Document object with metadata\n",
    "        doc = Document(text=doc_text, metadata={\"source\": filename})\n",
    "        \n",
    "        all_docs.append(doc)\n",
    "\n",
    "print(f\"Loaded {len(all_docs)} PDF documents.\")\n",
    "\n",
    "# Set up the embedding model and Settings\n",
    "embedder = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "Settings.llm = OpenAI(model=\"gpt-4o-mini\") # Note: Changed from \"gpt-4o-mini\" to \"gpt-4\" as \"gpt-4o-mini\" is not a standard model name\n",
    "Settings.embed_model = embedder\n",
    "\n",
    "# Create the sentence window node parser with default settings\n",
    "node_parser = SentenceWindowNodeParser.from_defaults(\n",
    "    window_size=3,\n",
    "    window_metadata_key=\"window\",\n",
    "    original_text_metadata_key=\"original_text\",\n",
    ")\n",
    "\n",
    "# Process all documents\n",
    "all_nodes = []\n",
    "for doc in all_docs:\n",
    "    nodes = node_parser.get_nodes_from_documents([doc])\n",
    "    all_nodes.extend(nodes)\n",
    "\n",
    "# Create the VectorStoreIndex\n",
    "sentence_index = VectorStoreIndex(all_nodes)\n",
    "\n",
    "print(\"VectorStoreIndex created successfully.\")\n",
    "\n",
    "# Optional: Persist the index to disk\n",
    "sentence_index.storage_context.persist(\"./storage\")\n",
    "\n",
    "print(\"Index persisted to disk.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🍩 https://wandb.ai/sanaz_team/output_ratt_EGFR/r/call/01918697-10d2-7d13-8b3c-f28507b60db1\n",
      "Inhibition of γ-secretase has been explored as a potential strategy for treating Alzheimer's disease (AD) due to its role in the production of amyloid β-peptide (Aβ), which is a key component of senile plaques associated with the disease. γ-Secretase mediates the final cleavage of amyloid precursor protein (APP), leading to the generation of Aβ. However, the use of γ-secretase inhibitors (GSIs) in clinical trials has been complicated by side effects, primarily due to the inhibition of Notch signaling, as γ-secretase also cleaves other substrates beyond APP. This dual role of γ-secretase presents challenges in developing effective therapies that target Aβ production without adversely affecting other critical signaling pathways.\n",
      "Source 1:\n",
      "We also discuss the chemical biology of γ -secretase, in which small molecule probes \n",
      "enabled structural and functional insights into γ -secretase before the emergence of high -resolution structural studies. \n",
      " Finally, we discuss the recent crystal structures of γ -secretase, which have provided valuable perspectives on substrate \n",
      "recognition and molecular mechanisms of small molecules.  We conclude that modulation of γ -secretase will be part \n",
      "of a new wave of AD therapeutics.\n",
      " Keywords:  γ-secretase, Alzheimer’s disease, Inhibitor, Modulator, Mechanism\n",
      "© The Author(s) 2021.  Open Access  This article is licensed under a Creative Commons Attribution 4.0 International License, which \n",
      "permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the \n",
      "original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made.  The images or \n",
      "other third party material in this article are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line \n",
      "to the material.  If material is not included in the article’s Creative Commons licence and your intended use is not permitted by statutory \n",
      "regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. \n",
      "\n",
      "\n",
      "Source 2:\n",
      "REVIEW ARTICLE OPENγ-Secretase in Alzheimer ’s disease\n",
      "Ji-Yeun Hur1✉\n",
      "© The Author(s) 2022\n",
      "Alzheimer ’s disease (AD) is caused by synaptic and neuronal loss in the brain.  One of the characteristic hallmarks of AD is senile\n",
      "plaques containing amyloid β-peptide (A β).  Aβis produced from amyloid precursor protein (APP) by sequential proteolytic\n",
      "cleavages by β-secretase and γ-secretase, and the polymerization of A βinto amyloid plaques is thought to be a key pathogenic\n",
      "event in AD.  Since γ-secretase mediates the ﬁnal cleavage that liberates A β,γ-secretase has been widely studied as a potential drug\n",
      "target for the treatment of AD.  γ-Secretase is a transmembrane protein complex containing presenilin, nicastrin, Aph-1, and Pen-2,\n",
      "which are suf ﬁcient for γ-secretase activity.  γ-Secretase cleaves >140 substrates, including APP and Notch.  Previously, γ-secretase\n",
      "inhibitors (GSIs) were shown to cause side effects in clinical trials due to the inhibition of Notch signaling. \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "query_engine= sentence_index.as_query_engine(\n",
    "    similarity_top_k=2,\n",
    "    # the target key defaults to `window` to match the node_parser's default\n",
    "    node_postprocessors=[\n",
    "        MetadataReplacementPostProcessor(target_metadata_key=\"window\")\n",
    "    ],\n",
    ")\n",
    "window_response = query_engine.query(\n",
    "    \"Inhibition of gamma secretase for Alzheimer's disease treatment\",\n",
    ")\n",
    "print(window_response)\n",
    "\n",
    "for i, node in enumerate(window_response.source_nodes):\n",
    "    print(f\"Source {i + 1}:\")\n",
    "    print(node.node.metadata['window'])  # or node.node.text, depending on how it's stored\n",
    "    print(\"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM_agent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
