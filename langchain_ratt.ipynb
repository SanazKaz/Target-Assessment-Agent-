{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "import asyncio  # Add this import\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import json\n",
    "from APIs.combinedapi import PubMedProcessor\n",
    "%autoawait asyncio\n",
    "import datetime\n",
    "from IPython.display import display, HTML\n",
    "from difflib import unified_diff\n",
    "import weave \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "openai_client = OpenAI(api_key=\"OPENAI_API_KEY\")\n",
    "load_dotenv()\n",
    "llm = ChatOpenAI()\n",
    "llm = ChatOpenAI(api_key=\"OPENAI_API_KEY\" )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_prompt = \"You are ChatGPT, a large language model trained by OpenAI, based on the GPT-4 architecture and you cite the papers you use in your answers using Harvard Style.\"\n",
    "\n",
    "working_hypothesis_prompt = \"\"\" \n",
    "# Scientific Rationale for 5-alpha reductase  in Huntington's Disease\n",
    "\n",
    "\n",
    "## Target Information \n",
    "### Develop a scientific rationale for the following:\n",
    "                             \n",
    "    **Given target:**  5-alpa reductase\n",
    "    **Given disease:** Huntington's disease\n",
    "    **Given mode of action:** Inhibition of 5-alpha reductase can promote neuronal survival and reduce mutant huntingtin protein aggregation\n",
    "\n",
    "## Task 1: Develop Scientific Rationale\n",
    "\n",
    "### Working Hypothesis\n",
    "- Detailed description of the idea\n",
    "- Unmet medical need\n",
    "- Suitability for combination therapy\n",
    "- Predictive biomarkers\n",
    "- Clinical relevance of existing biomarkers\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "clinical_target_prompt = \"\"\" #Clinical Target Rationale for 5-alpha reductase in Huntington's Disease\n",
    "\n",
    "\n",
    "## Target Information \n",
    "### Develop a scientific rationale for the following:\n",
    "                           \n",
    "    **Given target:** 5-alpa reductase\n",
    "    **Given disease:** Huntington's disease\n",
    "    **Given mode of action:** Inhibition of 5-alpha reductase can promote neuronal survival and reduce mutant huntingtin protein aggregation\n",
    "\n",
    "\n",
    " ### Clinical target rationale:\n",
    "    - How relevant is the target location to the disease biology?\n",
    "    - How is the target expression altered in human disease?\n",
    "    - How is the target involved in the physiological process relevant to the disease?\n",
    "    - Which phenotypes and genotypes were identified for the target?\n",
    "    - How is the genetic link between the target and the disease?\n",
    "    - Describe the evidence provided in clinics or by tools acting on the pathway where the target is involved.\n",
    "    - Which kind of target modulation is required to treat the disease? \"\"\"\n",
    "\n",
    "Challenges_prompt_1 = \"\"\" \n",
    "#Challenges for the drug discovery program related to 5-alpha reductase  in Huntington's disease\n",
    "                           \n",
    "## Target Information \n",
    "### Develop a scientific rationale for the following:\n",
    "                           \n",
    "    **Given target:**  5-alpa reductase\n",
    "    **Given disease:** Huntington's disease\n",
    "    **Given mode of action:** Inhibition of 5-alpha reductase can promote neuronal survival and reduce mutant huntingtin protein aggregation\n",
    "\n",
    "\n",
    "### Challenges:\n",
    "- Check the following idea for details on small molecule compounds: Developing small molecule modulators or inhibitors of gamma secretase for Alzheimer's disease treatment.\n",
    "- Is a 'information driven approach' (IDA) strategy based on available small molecules possible?\n",
    "- Which small molecular modulators of the target known?\n",
    "- Which inhibitors, antagonists, agonists, negative allosteric modulators (NAM), positive allosteric modulators (PAM) are required for target modulation in the given disease? \n",
    "\n",
    "\"\"\"\n",
    "Challenges_prompt_2 = \"\"\"\n",
    "\n",
    "#Challenges for the drug discovery program related to 5-alpha reductase  in Huntington's disease\n",
    "                           \n",
    "## Target Information \n",
    "### Develop a scientific rationale for the following:\n",
    "                           \n",
    "    **Given target:**  5-alpa reductase\n",
    "    **Given disease:** Huntington's disease\n",
    "    **Given mode of action:** Inhibition of 5-alpha reductase can promote neuronal survival and reduce mutant huntingtin protein aggregation\n",
    "\n",
    "- Which patients would respond the therapy?\n",
    "- Is the proposed mode of action on the target desirable and commercially viable in a clinical setting?\n",
    "- What are advantages and disadvantages of different therapeutic modalities (antibodies, small molecules, antisense oligonucleotides, PROTACs, molecular glue, peptide macrocycles, and so on) for tackling the target?\n",
    "\n",
    "- Alternative indications:\n",
    "- Describe alternative indication for modulators of the target and explain why.\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "all_prompts = [working_hypothesis_prompt, clinical_target_prompt, Challenges_prompt_1, Challenges_prompt_2]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import Dict, Any\n",
    "import openai\n",
    "import asyncio\n",
    "from typing import Dict, Any\n",
    "\n",
    "\n",
    "async def pubmed_paperqa(query: str) -> Dict[str, Any]:\n",
    "    \"\"\" Searches PubmedCentral for papers using a query\n",
    "    and returns the most relevant chunks using paperQA\"\"\"\n",
    "\n",
    "    max_attempts = 2\n",
    "    max_results: int = 4\n",
    "    pubmed_query = query\n",
    "    doc_query = query\n",
    "    email = \"sanazkazemi@hotmail.com\"\n",
    "    print(f\"pubmed_paperqa called with query: {query}, max_results: {max_results}\")\n",
    "\n",
    "    \n",
    "    pubmed_instance = PubMedProcessor(email)\n",
    "\n",
    "    # some error handeling in case the API call fails the algorithm will continue.\n",
    "    results_dict = {}\n",
    "    for attempt in range(max_attempts):\n",
    "        try:\n",
    "            results_dict = await pubmed_instance.full_process(pubmed_query, doc_query, max_results)\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"Error in attempt {attempt+1}: {e}\")\n",
    "\n",
    "    if results_dict is None:\n",
    "        print(\"All API call attempts failed. Continuing with empty results.\")\n",
    "        results_dict = {\"error\": \"API calls failed\", \"results\": []}\n",
    "\n",
    "\n",
    "    # saving the refs to file because the LLM cant do it as it edits in paragraphs - has to be manually done\n",
    "\n",
    "    file_path = 'RATT_refs.json'\n",
    "\n",
    "    try:\n",
    "        if os.path.exists(file_path) and os.path.getsize(file_path) > 0:\n",
    "            with open(file_path, 'r') as f:\n",
    "                existing_data = json.load(f)\n",
    "        else:\n",
    "            existing_data = []\n",
    "    except json.JSONDecodeError:\n",
    "        print(\"Error reading existing data. Starting with empty list.\")\n",
    "\n",
    "        existing_data = []\n",
    "\n",
    "    existing_data.append(results_dict)\n",
    "\n",
    "    with open(file_path, 'w') as f:\n",
    "        json.dump(existing_data, f, indent=4)\n",
    "\n",
    "                        \n",
    "    return json.dumps(results_dict, indent=4)\n",
    "\n",
    "# To run this in a Jupyter notebook cell:\n",
    "# query = \"Alzheimer's disease and gamma secretase\"\n",
    "# results = await pubmed_paperqa(query)\n",
    "# print(results)\n",
    "\n",
    "from difflib import unified_diff\n",
    "import html\n",
    "\n",
    "def generate_diff_html(text1, text2, fromfile='Original', tofile='Modified'):\n",
    "    diff = unified_diff(text1.splitlines(keepends=True),\n",
    "                        text2.splitlines(keepends=True),\n",
    "                        fromfile=fromfile, tofile=tofile, n=3)\n",
    "    \n",
    "    html_output = ['''\n",
    "    <style>\n",
    "        .diff-container {\n",
    "            font-family: monospace;\n",
    "            white-space: pre-wrap;\n",
    "            word-wrap: break-word;\n",
    "            background-color: #f8f9fa;\n",
    "            border: 1px solid #dee2e6;\n",
    "            border-radius: 4px;\n",
    "            padding: 10px;\n",
    "            margin-bottom: 20px;\n",
    "        }\n",
    "        .diff-header {\n",
    "            color: #6c757d;\n",
    "            margin-bottom: 10px;\n",
    "        }\n",
    "        .diff-add {\n",
    "            background-color: #e6ffec;\n",
    "            color: #24292e;\n",
    "        }\n",
    "        .diff-sub {\n",
    "            background-color: #ffebe9;\n",
    "            color: #24292e;\n",
    "        }\n",
    "        .diff-line {\n",
    "            display: block;\n",
    "            margin-bottom: 0;\n",
    "            padding: 2px 0;\n",
    "        }\n",
    "        .collapse-button {\n",
    "            background-color: #007bff;\n",
    "            color: white;\n",
    "            border: none;\n",
    "            padding: 5px 10px;\n",
    "            margin-bottom: 10px;\n",
    "            cursor: pointer;\n",
    "            border-radius: 4px;\n",
    "        }\n",
    "        .hidden {\n",
    "            display: none;\n",
    "        }\n",
    "    </style>\n",
    "    <div class=\"diff-container\">\n",
    "    <button class=\"collapse-button\" onclick=\"toggleDiff(this)\">Collapse/Expand Diff</button>\n",
    "    <div class=\"diff-content\">\n",
    "    ''']\n",
    "    \n",
    "    for line in diff:\n",
    "        if line.startswith('---') or line.startswith('+++'):\n",
    "            html_output.append(f'<div class=\"diff-header\">{html.escape(line)}</div>')\n",
    "        elif line.startswith('+'):\n",
    "            html_output.append(f'<span class=\"diff-line diff-add\">{html.escape(line)}</span>')\n",
    "        elif line.startswith('-'):\n",
    "            html_output.append(f'<span class=\"diff-line diff-sub\">{html.escape(line)}</span>')\n",
    "        else:\n",
    "            html_output.append(f'<span class=\"diff-line\">{html.escape(line)}</span>')\n",
    "    \n",
    "    html_output.append('''\n",
    "    </div>\n",
    "    </div>\n",
    "    <script>\n",
    "    function toggleDiff(button) {\n",
    "        var content = button.nextElementSibling;\n",
    "        if (content.style.display === \"none\") {\n",
    "            content.style.display = \"block\";\n",
    "            button.textContent = \"Collapse Diff\";\n",
    "        } else {\n",
    "            content.style.display = \"none\";\n",
    "            button.textContent = \"Expand Diff\";\n",
    "        }\n",
    "    }\n",
    "    </script>\n",
    "    ''')\n",
    "    \n",
    "    return ''.join(html_output)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def pick_best_query(queries: list, question: str, answer: str) -> str:\n",
    "    \"\"\"picks the best query from the list of queries\"\"\"\n",
    "\n",
    "    gpt_prompt =\"\"\"\n",
    "    for the given question and answer, pick the best query \n",
    "    from the list of queries that you think is most relevant especially to the last few sentences of the answer.\n",
    "    ## IMPORTANT:\n",
    "    Just return the best query. Do not add any additional information.\n",
    "    \"\"\"\n",
    "\n",
    "    best_query = openai.chat.completions.create(\n",
    "\n",
    "     model = \"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"you are a scientific researcher, you are tasked with finding the best query to search for scientific papers on PubMed.\"            \n",
    "        },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"##Instruction:{gpt_prompt}\\n\\n###Question: {question}\\n\\n###Question:{answer}\\n\\n##Queries: {[queries]}\\n\\n\"\n",
    "            }\n",
    "        ],\n",
    "        temperature=1 # here you can adjust the temperature to get more or less creative search terms\n",
    "    ).choices[0].message.content\n",
    "\n",
    "    return best_query\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_query(question, answer, num_queries) -> str:\n",
    "\n",
    "    \"\"\"Generates queries to search for in Pubmed based on the question\"\"\"    \n",
    "    query_prompt = \"\"\" You are a scientific researcher, \n",
    "                        you are tasked with finding the best query to search \n",
    "                        for scientific papers on PubMed.\n",
    "                                                    \n",
    "                            I want to verify the content correctness of the given answer especially the last few sentences.\n",
    "                            Please summarize the content with the corresponding question.\n",
    "                            This summarization will be used as a query to search with Bing search engine.\n",
    "                            The query should be short but needs to be specific to promise Bing can find related knowledge or pages.\n",
    "                            You can also use search syntax to make the query short and clear enough for the search engine to find relevant language data.\n",
    "                            Try to make the query as relevant as possible to the last few setences of the the answer provided.\n",
    "                            **IMPORTANT**\n",
    "                            Just output the query directly. DO NOT add additional explanations or introducement in the answer unless you are asked to.\n",
    "\n",
    "                        The following worked very well for me in the past in terms of generating the highest number of results use it as a guide:\n",
    "                        ###Example:\n",
    "                        \"{Target}\" AND \"{Disease}\" AND (\"{relevant_keyword}\" OR \"{relevant_keyword_1} Or \"{relevant_keyword_n}\")\" and so on.\n",
    "                        ##IMPORTANT:\n",
    "                        Just provide the query. Do not add any additional information.\n",
    "                        DO NOT copy the given example\"\"\"\n",
    "    \n",
    "\n",
    "    queries = []\n",
    "\n",
    "    for i in range(num_queries):\n",
    "        try:\n",
    "            query = openai.chat.completions.create(\n",
    "                model=\"gpt-4o-mini\",\n",
    "                messages=[\n",
    "                    {\n",
    "                        \"role\": \"system\",\n",
    "                        \"content\": \"You are ChatGPT, a large language model trained by OpenAI, based on the GPT-4 architecture.\"\n",
    "\n",
    "                    },\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": f\"##Question: {question}\\n\\n##Content: {answer}\\n\\n##Instruction: {query_prompt}\"\n",
    "                    }\n",
    "                ],\n",
    "                temperature=1\n",
    "            ).choices[0].message.content\n",
    "\n",
    "            print(f\"query {i}: {query}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"error {e}\")\n",
    "        queries.append(query)\n",
    "\n",
    "    best_query = pick_best_query(queries,answer, question)\n",
    "    print(f\"best query: {best_query}\")\n",
    "\n",
    "    return best_query\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "async def main(question: str, answer, num_queries: int):\n",
    "    \"\"\"Main function to get the best query for the question\"\"\"\n",
    "    \n",
    "    \n",
    "    search_query = get_query(question, answer, num_queries)\n",
    "    \n",
    "    # Remove only the outermost single quotes if they exist otherwise doesnt work - not elegant but works\n",
    "    if search_query.startswith(\"'\") and search_query.endswith(\"'\"):\n",
    "        cleaned_query = search_query[1:-1]\n",
    "    else:\n",
    "        cleaned_query = search_query\n",
    "    \n",
    "    # Replace escaped single quotes with regular single quotes\n",
    "    cleaned_query = cleaned_query.replace(\"\\\\'\", \"'\")\n",
    "    \n",
    "    print(f\"Cleaned search query: {cleaned_query}\")\n",
    "    \n",
    "    results = await pubmed_paperqa(cleaned_query)\n",
    "\n",
    "    print(results)  # This is the final output\n",
    "\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = \"\"\" notch signalling is relevant to GSEC development and drug discovery in Alzheimer's disease.\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = await main(answer=answer, question = working_hypothesis_prompt, num_queries=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_agents = 1 \n",
    "num_steps = 3\n",
    "final_output_mode = 'final_step_only'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "def COT_agent(question):\n",
    "    \"\"\"Generates a chain of thought answer for comparison to RATT.\n",
    "    question: str: the prompt to answer\n",
    "    draft_prompt: str: the prompt to generate the draft\n",
    "    system_prompt: str: the prompt to generate the system message\"\"\"\n",
    "\n",
    "    draft_prompt = '''\n",
    "IMPORTANT:\n",
    "Try to answer this question/instruction with step-by-step thoughts and make the answer more structured.\n",
    "Use `\\n\\n` to split the answer into several paragraphs.\n",
    "Just respond to the instruction directly. DO NOT add additional explanations or introducement in the answer unless you are asked to.\n",
    "'''\n",
    "\n",
    "    # Loop to generate different initial answers\n",
    "    COT_draft = openai.chat.completions.create(\n",
    "         model=\"gpt-4o-mini\",\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": chat_prompt # should this be scientific rationale prompt or something less specific?\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": question + draft_prompt\n",
    "                }\n",
    "            ],\n",
    "            temperature=0.5\n",
    "        ).choices[0].message.content\n",
    "\n",
    "    return COT_draft\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "\n",
    "def split_draft(draft, split_char='\\n\\n'):\n",
    "    # split_char: '\\n\\n'\n",
    "    draft_paragraphs = draft.split(split_char)\n",
    "    # print(f\"The draft answer has {len(draft_paragraphs)}\")\n",
    "    return draft_paragraphs\n",
    "\n",
    "\n",
    "def get_revise_answer(question, answer, retrieved_data):\n",
    "    revise_prompt = '''\n",
    "I want to revise the answer according to retrieved related text of the question from Pubmed central. You will receive the summary, full text, relevance score to the query, and the full reference. If you use any of the given articles, you **must** reference using the Harvard style with DOI added and add the full citation to the top of the answer. \n",
    "**DO NOT REMOVE OR ALTER ANY CITATIONS FROM THE TOP OF THE ANSWER.**\n",
    "You need to check whether the answer is correct.\n",
    "If you find some errors in the answer, revise the answer to make it better.\n",
    "If you find some necessary details are ignored, add it to make the answer more plausible according to the related text.\n",
    "If you find that a part of the answer is correct and does not require any additional details, maintain that part of the answer unchanged. Directly output the original content of that part without any modifications.\n",
    "**IMPORTANT**\n",
    "Try to keep the structure (multiple paragraphs with its subtitles) in the revised answer and make it more structual for understanding.\n",
    "Split the paragraphs with `\\n\\n` characters.\n",
    "Just output the revised answer directly. DO NOT add additional explanations or annoucement in the revised answer unless you are asked to.\n",
    "'''\n",
    "    revised_answer = openai.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": chat_prompt\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"##Pubmed central retrieved articles: {retrieved_data}\\n\\n##Question: {question}\\n\\n##previous Answer: {answer}\\n\\n##Instruction: {revise_prompt}\"\n",
    "            }\n",
    "        ],\n",
    "        temperature=0.5\n",
    "    ).choices[0].message.content\n",
    "    \n",
    "    \n",
    "    return revised_answer\n",
    "\n",
    "\n",
    "\n",
    "async def RAG(question, draft_paragraphs):\n",
    "    \"\"\" args:\n",
    "    question: str: the prompt to answer\n",
    "    draft_paragraphs: list: the list of paragraphs from the initial n drafts\n",
    "    \"\"\"\n",
    "    answer = \"\"\n",
    "\n",
    "    for i, paragraph in enumerate(draft_paragraphs):\n",
    "        answer += '\\n\\n' + paragraph\n",
    "\n",
    "        api_response = await main(question, answer, num_queries=2)  # Now using the entire answer instead of just the paragraph\n",
    "\n",
    "        revised_answer = get_revise_answer(question, answer, api_response)  # Using the entire answer\n",
    "        if revised_answer != answer:\n",
    "            diff_html = generate_diff_html(answer, revised_answer)\n",
    "            display(HTML(diff_html))\n",
    "            answer = revised_answer\n",
    "        \n",
    "        print(f\"Completed iteration {i+1}/{len(draft_paragraphs)}\")\n",
    "\n",
    "        print('+'* 80 + '\\n\\n')\n",
    "        print(f\"RESULT OF PUBMED API:\\n{answer}\")\n",
    "    \n",
    "    return answer\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "async def get_draft_tot_initial(question: str, num_agents: int):\n",
    "    \"\"\"Generates initial answers from multiple agents for comparison\"\"\"\n",
    "    draft_prompt = \"\"\"\n",
    "            IMPORTANT:\n",
    "            Try to answer this question/instruction with step-by-step thoughts and make the answer more structured.\n",
    "            Use `\\n\\n` to split the answer into several paragraphs.\n",
    "            Just respond to the instruction directly. DO NOT add additional explanations or introducement in the answer unless you are asked to.\n",
    "            \"\"\"\n",
    "\n",
    "    refine_prompt = \"\"\"\n",
    "            Maintaining *ALL* citations and references and referencing the answers provided by all agents, synthesize a more detailed and comprehensive response by integrating all relevant details from these answers. \n",
    "            Ensure logical coherence and provide ONLY THE MERGED ANSWER AND CITATIONS AS THE OUTPUT, omitting any discussion of the comparison process or analytical thoughts.\"\"\"\n",
    "\n",
    "    agent_drafts = []\n",
    "    for i in range(num_agents):\n",
    "        draft = openai.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": chat_prompt\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": question + draft_prompt\n",
    "                }\n",
    "            ],\n",
    "            temperature=0.5\n",
    "        ).choices[0].message.content\n",
    "        print(f\"####################draft {i}: {draft}########################################\")\n",
    "\n",
    "        print(\"Processing draft...\")\n",
    "        draft_paragraphs = split_draft(draft)\n",
    "\n",
    "        draft_modified = await RAG(question, draft_paragraphs)\n",
    "\n",
    "        agent_drafts.append(f\"Agent{i+1}: {draft_modified}\")\n",
    "\n",
    "        print(f\"[INFO] Agent{i + 1}/{num_agents} retrieved draft...\")\n",
    "\n",
    "        agent_input = '\\n\\n'.join(agent_drafts) + '\\n\\n' + refine_prompt\n",
    "\n",
    "        final_draft = openai.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": chat_prompt\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": agent_input\n",
    "                }\n",
    "            ],\n",
    "            temperature=0.5\n",
    "        ).choices[0].message.content\n",
    "\n",
    "        print(f\"{datetime.now()} - Final draft: {final_draft}\")\n",
    "\n",
    "\n",
    "    return final_draft\n",
    "\n",
    "# FIX BELOW FUNCTION NOT USING PREVIOUS ANSWER ?? ALSO STRING CHAR LIST WITHIN LIST - SHOULD BE STRING\n",
    "\n",
    "\n",
    "async def get_draft_tot(question, previous_answer, num_agents):\n",
    "\n",
    "    draft_prompt = f\"\"\" Base your response on the provided question and the previous answer. Expand the answer by adding more details to enhance its comprehensiveness. Ensure that the expansion maintains logical coherence and enriches the details, making the response more thorough and well-structured.\n",
    "        Question: {question}\n",
    "        Previous Answer: {previous_answer}\n",
    "        IMPORTANT:\n",
    "        DO NOT REMOVE ANY CITATIONS OR REFERENCES IN THE ANSWER if you use any citations or references in the answer - you must reference in Harvard style + the DOI in the answer and add the citation to the top of the answer.\n",
    "        Answer the full question with step-by-step thoughts and make the answer more structural.\n",
    "        Use `\\n\\n` to split the answer into several paragraphs.\n",
    "        Just respond to the instruction directly. DO NOT add additional explanations or introducement in the answer unless you are asked to. \"\"\"\n",
    "\n",
    "    refine_prompt = \"\"\"Maintaining *ALL* citations and references and referencing the answers provided by all agents, synthesize a more detailed and comprehensive response by integrating all relevant details from these answers. \n",
    "            Ensure logical coherence and provide ONLY THE MERGED ANSWER AND CITATIONS AS THE OUTPUT, omitting any discussion of the comparison process or analytical thoughts.\"\"\"\n",
    "\n",
    "    agents_drafts = []\n",
    "    for i in range(num_agents):\n",
    "        draft = openai.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": chat_prompt\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": draft_prompt\n",
    "                }\n",
    "            ],\n",
    "            temperature=0.5\n",
    "        ).choices[0].message.content\n",
    "\n",
    "        draft_paragraphs = split_draft(draft)\n",
    "\n",
    "        draft_modified = await RAG(question, draft_paragraphs)\n",
    "\n",
    "        agents_drafts.append(f\"Agent{i+1}: {draft_modified}\")\n",
    "    \n",
    "    agents_input = '\\n\\n'.join(agents_drafts) + '\\n\\n' + refine_prompt\n",
    "\n",
    "    final_draft_raw = openai.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": chat_prompt\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": agents_input\n",
    "            }\n",
    "        ],\n",
    "        temperature=0.5\n",
    "    ).choices[0].message.content\n",
    "\n",
    "    print(f\"##########Final draft raw #########################: {final_draft_raw}...\")\n",
    "\n",
    "    revise_prompt = \"\"\"\n",
    "            Based on the original answer and an additional supplementary answer, generate a response that is richer in detail and logically coherent. MAINTAIN ALL REFERENCES AND CITATIONS IN THE ANSWER. Do not remove ANY citations.\n",
    "            Review the original answer:\n",
    "        1. If any part of the answer is correct and requires no further details, retain that portion unchanged and output it directly as it is.\n",
    "        2. For parts that may be improved or lack necessary details, enhance them by integrating information from the supplementary answer to make the response more comprehensive and accurate, reference and cite the full reference at the top where neccessary.\n",
    "        3. If you identify any errors within the answers, correct these errors while ensuring that the revised content remains logically coherent - check this against the retreived articles.\n",
    "        Original Answer: {previous_answer}\n",
    "        Supplementary Answer: {final_draft_raw}\n",
    "\n",
    "        **IMPORTANT**\n",
    "        Ensure the revised answer maintains a structured format with paragraphs and subtitles for better clarity. \n",
    "        Separate the paragraphs with `\\n\\n` characters. Output only the enhanced answer directly and the references and citations if any, without any extra explanations or announcements unless specifically requested.\"\"\"\n",
    "    \n",
    "    final_draft = openai.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": chat_prompt\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": revise_prompt\n",
    "            }\n",
    "        ],\n",
    "        temperature=0.5\n",
    "    ).choices[0].message.content\n",
    "\n",
    "    return final_draft\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "\n",
    "async def ratt(question, num_agents):\n",
    "    step_num = num_steps\n",
    "    print(f\"{datetime.now()} [INFO] Retrieving Step 1 draft...\")\n",
    "\n",
    "    draft = await get_draft_tot_initial(question,num_agents)\n",
    "    \n",
    "    print(f\"{datetime.now()} [INFO] Step 1 draft returned\")\n",
    "    print(f\"##################### DRAFT #######################\")\n",
    "    print(draft)\n",
    "    print(f\"#####################  END  #######################\")\n",
    "\n",
    "    print(f\"{datetime.now()} [INFO] Processing draft...\")\n",
    "    draft_paragraphs = split_draft(draft)\n",
    "    print(f\"{datetime.now()} [INFO] Draft split into {len(draft_paragraphs)} parts\")\n",
    "\n",
    "    answer_first_state = await RAG(question, draft_paragraphs)\n",
    "\n",
    "    previous_answer = answer_first_state\n",
    "\n",
    "    each_step_drafts = [f\"Step 1 \\n: {previous_answer}\"]\n",
    "\n",
    "    for iteration in range(1, step_num):\n",
    "        print(f\"{datetime.now()} [INFO] Retrieving Step {iteration + 1} draft...\")\n",
    "        draft = await get_draft_tot(question, previous_answer, num_agents=num_agents)\n",
    "        print(f\"{datetime.now()} [INFO] Step {iteration + 1} draft returned\")\n",
    "        print(f\"##################### DRAFT #######################\")\n",
    "        print(draft)\n",
    "        print(f\"#####################  END  #######################\")\n",
    "\n",
    "        print(f\"{datetime.now()} [INFO] Processing draft...\")\n",
    "        draft_paragraphs = split_draft(draft)\n",
    "        print(f\"{datetime.now()} [INFO] Draft split into {len(draft_paragraphs)} parts\")\n",
    "\n",
    "        # filtered_paragraphs = filter_paragraphs(draft_paragraphs, iteration, step_num)\n",
    "        final_answer = await RAG(question, draft_paragraphs)\n",
    "\n",
    "        each_step_drafts.append(f\"Step {iteration + 1} \\n: {final_answer}\")\n",
    "\n",
    "        # Update previous_answer for the current iteration's response\n",
    "        previous_answer = final_answer\n",
    "\n",
    "    draft_cot = COT_agent(question) # for comparison\n",
    "\n",
    "    if final_output_mode == 'combine_each_step':\n",
    "        final_draft = '\\n\\n'.join(each_step_drafts)\n",
    "        refine_prompt = f\"\"\"\n",
    "            Maintaining *ALL* citations and references and referencing the answers provided by all agents, synthesize a more detailed and comprehensive response by integrating all relevant details from these answers. \n",
    "            Ensure logical coherence and provide ONLY THE MERGED ANSWER AND CITATIONS AS THE OUTPUT, omitting any discussion of the comparison process or analytical thoughts.\"\"\"\n",
    "        previous_answer = openai.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": chat_prompt\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": final_draft + '\\n\\n' + refine_prompt\n",
    "                }\n",
    "            ],\n",
    "            temperature=0.5 ## ? why\n",
    "        ).choices[0].message.content\n",
    "\n",
    "    return draft_cot, previous_answer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import weave\n",
    "from weave import Evaluation\n",
    "import json\n",
    "import aiofiles\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# answer_cot, answer_ratt = ratt(chatgpt_prompt, num_agents=1)\n",
    "\n",
    "@weave.op()\n",
    "async def main_ratt(question, num_agents, output_file=\"text_ratt/EGFR/EGFR.json\"):\n",
    "    answer_cot, answer_ratt = await ratt(question, num_agents)\n",
    "\n",
    "    result = {\n",
    "        \"prompt\": question,\n",
    "        \"output_cot\": answer_cot, \n",
    "        \"output_ratt\": answer_ratt\n",
    "    }\n",
    "\n",
    "    # Write the result to a file\n",
    "    async with aiofiles.open(output_file, mode='a') as f:\n",
    "        await f.write(json.dumps(result) + \"\\n\")\n",
    "\n",
    "    return result\n",
    "\n",
    "weave.init('output_ratt_EGFR')\n",
    "\n",
    "result_2 = await main_ratt(Challenges_prompt_2, num_agents=1)\n",
    "# weave.init('output_ratt_bace')\n",
    "# for prompt in all_prompts:\n",
    "#     print(f\"**************************{prompt}****************************\")\n",
    "#     results = asyncio.run(main_ratt(prompt, num_agents=1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM_agent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
